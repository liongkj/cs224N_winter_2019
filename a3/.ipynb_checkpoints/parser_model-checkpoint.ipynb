{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from collections import Counter\n",
    "from utils.general_utils import get_minibatches\n",
    "from ipynb.fs.full.parser_transitions import minibatch_parse\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserModel(nn.Module):\n",
    "    \"\"\" Feedforward neural network with an embedding layer and single hidden layer.\n",
    "    The ParserModel will predict which transition should be applied to a\n",
    "    given partial parse configuration.\n",
    "\n",
    "    PyTorch Notes:\n",
    "        - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In PyTorch all neural networks\n",
    "            are a subclass of this \"nn.Module\".\n",
    "        - The \"__init__\" method is where you define all the layers and their respective parameters\n",
    "            (embedding layers, linear layers, dropout layers, etc.).\n",
    "        - \"__init__\" gets automatically called when you create a new instance of your class, e.g.\n",
    "            when you write \"m = ParserModel()\".\n",
    "        - Other methods of ParserModel can access variables that have \"self.\" prefix. Thus,\n",
    "            you should add the \"self.\" prefix layers, values, etc. that you want to utilize\n",
    "            in other ParserModel methods.\n",
    "        - For further documentation on \"nn.Module\" please see https://pytorch.org/docs/stable/nn.html.\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings, n_features=36,\n",
    "        hidden_size=200, n_classes=3, dropout_prob=0.5):\n",
    "        \"\"\" Initialize the parser model.\n",
    "\n",
    "        @param embeddings (Tensor): word embeddings (num_words, embedding_size)\n",
    "        @param n_features (int): number of input features\n",
    "        @param hidden_size (int): number of hidden units\n",
    "        @param n_classes (int): number of output classes\n",
    "        @param dropout_prob (float): dropout probability\n",
    "        \"\"\"\n",
    "        super(ParserModel, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.embed_size = embeddings.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
    "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "\n",
    "        ### YOUR CODE HERE (~5 Lines)\n",
    "        self.embed_to_hidden = nn.Linear(self.embed_size * self.n_features, self.hidden_size)\n",
    "        nn.init.xavier_uniform_(self.embed_to_hidden.weight, gain = 1.0) \n",
    "        self.dropout = nn.Dropout(p = dropout_prob)\n",
    "        self.hidden_to_logits = nn.Linear(self.hidden_size, self.n_classes) #hidden layer to score\n",
    "        nn.init.xavier_uniform_(self.hidden_to_logits.weight, gain = 1.0)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def embedding_lookup(self, t):\n",
    "        \"\"\" Utilize `self.pretrained_embeddings` to map input `t` from input tokens (integers)\n",
    "            to embedding vectors.\n",
    "\n",
    "            PyTorch Notes:\n",
    "                - `self.pretrained_embeddings` is a torch.nn.Embedding object that we defined in __init__\n",
    "                - Here `t` is a tensor where each row represents a list of features. Each feature is represented by an integer (input token).\n",
    "                - In PyTorch the Embedding object, e.g. `self.pretrained_embeddings`, allows you to\n",
    "                    go from an index to embedding. Please see the documentation (https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding)\n",
    "                    to learn how to use `self.pretrained_embeddings` to extract the embeddings for your tensor `t`.\n",
    "\n",
    "            @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
    "\n",
    "            @return x (Tensor): tensor of embeddings for words represented in t\n",
    "                                (batch_size, n_features * embed_size)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE (~1-3 Lines)\n",
    "        ### TODO:\n",
    "        ###     1) Use `self.pretrained_embeddings` to lookup the embeddings for the input tokens in `t`.\n",
    "        ###     2) After you apply the embedding lookup, you will have a tensor shape (batch_size, n_features, embedding_size).\n",
    "        ###         Use the tensor `view` method to reshape the embeddings tensor to (batch_size, n_features * embedding_size)\n",
    "        ###\n",
    "        ### Note: In order to get batch_size, you may need use the tensor .size() function:\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size\n",
    "        ###\n",
    "        ###  Please see the following docs for support:\n",
    "        ###     Embedding Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
    "        ###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "        \n",
    "        # find a list of features of tensor t and reshape to tensor with embed size\n",
    "        # view is like change size\n",
    "        # -1 is put how big is the column of t\n",
    "        x = self.pretrained_embeddings(t).view(t.size(0),-1)\n",
    "        ### END YOUR CODE\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\" Run the model forward.\n",
    "\n",
    "            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss\n",
    "\n",
    "            PyTorch Notes:\n",
    "                - Every nn.Module object (PyTorch model) has a `forward` function.\n",
    "                - When you apply your nn.Module to an input tensor `t` this function is applied to the tensor.\n",
    "                    For example, if you created an instance of your ParserModel and applied it to some `t` as follows,\n",
    "                    the `forward` function would called on `t` and the result would be stored in the `output` variable:\n",
    "                        model = ParserModel()\n",
    "                        output = model(t) # this calls the forward function\n",
    "                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n",
    "\n",
    "        @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
    "\n",
    "        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)\n",
    "                                 without applying softmax (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        ###  YOUR CODE HERE (~3-5 lines)\n",
    "        ### TODO:\n",
    "        ###     1) Apply `self.embedding_lookup` to `t` to get the embeddings\n",
    "        ###     2) Apply `embed_to_hidden` linear layer to the embeddings\n",
    "        ###     3) Apply relu non-linearity to the output of step 2 to get the hidden units.\n",
    "        ###     4) Apply dropout layer to the output of step 3.\n",
    "        ###     5) Apply `hidden_to_logits` layer to the output of step 4 to get the logits.\n",
    "        ###\n",
    "        ### Note: We do not apply the softmax to the logits here, because\n",
    "        ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.\n",
    "        ###\n",
    "        ### Please see the following docs for support:\n",
    "        ###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu\n",
    "        embeddings = self.embedding_lookup(t)\n",
    "        hidden = F.relu(self.embed_to_hidden(embeddings))\n",
    "        hidden_drop = self.dropout(hidden)\n",
    "        logits = self.hidden_to_logits(hidden_drop)\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Primary Functions\n",
    "# -----------------\n",
    "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n",
    "    \"\"\" Train the neural dependency parser.\n",
    "\n",
    "    @param parser (Parser): Neural Dependency Parser\n",
    "    @param train_data ():\n",
    "    @param dev_data ():\n",
    "    @param output_path (str): Path to which model weights and results are written.\n",
    "    @param batch_size (int): Number of examples in a single batch\n",
    "    @param n_epochs (int): Number of training epochs\n",
    "    @param lr (float): Learning rate\n",
    "    \"\"\"\n",
    "    best_dev_UAS = 0\n",
    "\n",
    "\n",
    "    ### YOUR CODE HERE (~2-7 lines)\n",
    "    ### TODO:\n",
    "    ###      1) Construct Adam Optimizer in variable `optimizer`\n",
    "    ###      2) Construct the Cross Entropy Loss Function in variable `loss_func`\n",
    "    ###\n",
    "    ### Hint: Use `parser.model.parameters()` to pass from model to optimizer\n",
    "    ###       necessary parameters to tune.\n",
    "    ### Please see the following docs for support:\n",
    "    ###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html\n",
    "    ###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss   \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(parser.model.parameters(),lr = lr)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
    "        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)\n",
    "        if dev_UAS > best_dev_UAS:\n",
    "            best_dev_UAS = dev_UAS\n",
    "            print(\"New best dev UAS! Saving model.\")\n",
    "            torch.save(parser.model.state_dict(), output_path)\n",
    "        print(\"\")\n",
    "\n",
    "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n",
    "    \"\"\" Train the neural dependency parser for single epoch.\n",
    "\n",
    "    Note: In PyTorch we can signify train versus test and automatically have\n",
    "    the Dropout Layer applied and removed, accordingly, by specifying\n",
    "    whether we are training, `model.train()`, or evaluating, `model.eval()`\n",
    "\n",
    "    @param parser (Parser): Neural Dependency Parser\n",
    "    @param train_data ():\n",
    "    @param dev_data ():\n",
    "    @param optimizer (nn.Optimizer): Adam Optimizer\n",
    "    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function\n",
    "    @param batch_size (int): batch size\n",
    "    @param lr (float): learning rate\n",
    "\n",
    "    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data\n",
    "    \"\"\"\n",
    "    parser.model.train() # Places model in \"train\" mode, i.e. apply dropout layer\n",
    "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    with tqdm(total=(n_minibatches)) as prog:\n",
    "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
    "            optimizer.zero_grad()   # remove any baggage in the optimizer\n",
    "            loss = 0. # store loss for this batch here\n",
    "            train_x = torch.from_numpy(train_x).long()\n",
    "            train_y = torch.from_numpy(train_y.nonzero()[1]).long()\n",
    "\n",
    "            ### YOUR CODE HERE (~5-10 lines)\n",
    "            ### TODO:\n",
    "            ###      1) Run train_x forward through model to produce `logits`\n",
    "            ###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.\n",
    "            ###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss\n",
    "            ###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)\n",
    "            ###         are the predictions (y^ from the PDF).\n",
    "            ###      3) Backprop losses\n",
    "            ###      4) Take step with the optimizer\n",
    "            ### Please see the following docs for support:\n",
    "            ###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step\n",
    "            logits = parser.model(train_x)\n",
    "            loss = loss_func(logits,train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ### END YOUR CODE\n",
    "            prog.update(1)\n",
    "            loss_meter.update(loss.item())\n",
    "\n",
    "    print (\"Average Train Loss: {}\".format(loss_meter.avg))\n",
    "\n",
    "    print(\"Evaluating on dev set\",)\n",
    "    parser.model.eval() # Places model in \"eval\" mode, i.e. don't apply dropout layer\n",
    "    dev_UAS, _ = parser.parse(dev_data)\n",
    "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
    "    return dev_UAS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parser_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "CS224N 2018-19: Homework 3\n",
    "parser_utils.py: Utilities for training the dependency parser.\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "P_PREFIX = '<p>:'\n",
    "L_PREFIX = '<l>:'\n",
    "UNK = '<UNK>'\n",
    "NULL = '<NULL>'\n",
    "ROOT = '<ROOT>'\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    language = 'english'\n",
    "    with_punct = True\n",
    "    unlabeled = True\n",
    "    lowercase = True\n",
    "    use_pos = True\n",
    "    use_dep = True\n",
    "    use_dep = use_dep and (not unlabeled)\n",
    "    data_path = './data'\n",
    "    train_file = 'train.conll'\n",
    "    dev_file = 'dev.conll'\n",
    "    test_file = 'test.conll'\n",
    "    embedding_file = './data/en-cw.txt'\n",
    "\n",
    "\n",
    "class Parser(object):\n",
    "    \"\"\"Contains everything needed for transition-based dependency parsing except for the model\"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        root_labels = list([l for ex in dataset\n",
    "                           for (h, l) in zip(ex['head'], ex['label']) if h == 0])\n",
    "        counter = Counter(root_labels)\n",
    "        if len(counter) > 1:\n",
    "            logging.info('Warning: more than one root label')\n",
    "            logging.info(counter)\n",
    "        self.root_label = counter.most_common()[0][0]\n",
    "        deprel = [self.root_label] + list(set([w for ex in dataset\n",
    "                                               for w in ex['label']\n",
    "                                               if w != self.root_label]))\n",
    "        tok2id = {L_PREFIX + l: i for (i, l) in enumerate(deprel)}\n",
    "        tok2id[L_PREFIX + NULL] = self.L_NULL = len(tok2id)\n",
    "\n",
    "        config = Config()\n",
    "        self.unlabeled = config.unlabeled\n",
    "        self.with_punct = config.with_punct\n",
    "        self.use_pos = config.use_pos\n",
    "        self.use_dep = config.use_dep\n",
    "        self.language = config.language\n",
    "\n",
    "        if self.unlabeled:\n",
    "            trans = ['L', 'R', 'S']\n",
    "            self.n_deprel = 1\n",
    "        else:\n",
    "            trans = ['L-' + l for l in deprel] + ['R-' + l for l in deprel] + ['S']\n",
    "            self.n_deprel = len(deprel)\n",
    "\n",
    "        self.n_trans = len(trans)\n",
    "        self.tran2id = {t: i for (i, t) in enumerate(trans)}\n",
    "        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
    "\n",
    "        # logging.info('Build dictionary for part-of-speech tags.')\n",
    "        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[P_PREFIX + UNK] = self.P_UNK = len(tok2id)\n",
    "        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
    "        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
    "\n",
    "        # logging.info('Build dictionary for words.')\n",
    "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[UNK] = self.UNK = len(tok2id)\n",
    "        tok2id[NULL] = self.NULL = len(tok2id)\n",
    "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
    "\n",
    "        self.tok2id = tok2id\n",
    "        self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
    "\n",
    "        self.n_features = 18 + (18 if config.use_pos else 0) + (12 if config.use_dep else 0)\n",
    "        self.n_tokens = len(tok2id)\n",
    "\n",
    "    def vectorize(self, examples):\n",
    "        vec_examples = []\n",
    "        for ex in examples:\n",
    "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
    "                                  else self.UNK for w in ex['word']]\n",
    "            pos = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
    "                                   else self.P_UNK for w in ex['pos']]\n",
    "            head = [-1] + ex['head']\n",
    "            label = [-1] + [self.tok2id[L_PREFIX + w] if L_PREFIX + w in self.tok2id\n",
    "                            else -1 for w in ex['label']]\n",
    "            vec_examples.append({'word': word, 'pos': pos,\n",
    "                                 'head': head, 'label': label})\n",
    "        return vec_examples\n",
    "\n",
    "    def extract_features(self, stack, buf, arcs, ex):\n",
    "        if stack[0] == \"ROOT\":\n",
    "            stack[0] = 0\n",
    "\n",
    "        def get_lc(k):\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
    "\n",
    "        def get_rc(k):\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
    "                          reverse=True)\n",
    "\n",
    "        p_features = []\n",
    "        l_features = []\n",
    "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
    "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
    "        if self.use_pos:\n",
    "            p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
    "            p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
    "\n",
    "        for i in range(2):\n",
    "            if i < len(stack):\n",
    "                k = stack[-i-1]\n",
    "                lc = get_lc(k)\n",
    "                rc = get_rc(k)\n",
    "                llc = get_lc(lc[0]) if len(lc) > 0 else []\n",
    "                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n",
    "\n",
    "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
    "\n",
    "                if self.use_pos:\n",
    "                    p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
    "\n",
    "                if self.use_dep:\n",
    "                    l_features.append(ex['label'][lc[0]] if len(lc) > 0 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][rc[0]] if len(rc) > 0 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][lc[1]] if len(lc) > 1 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][rc[1]] if len(rc) > 1 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][llc[0]] if len(llc) > 0 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][rrc[0]] if len(rrc) > 0 else self.L_NULL)\n",
    "            else:\n",
    "                features += [self.NULL] * 6\n",
    "                if self.use_pos:\n",
    "                    p_features += [self.P_NULL] * 6\n",
    "                if self.use_dep:\n",
    "                    l_features += [self.L_NULL] * 6\n",
    "\n",
    "        features += p_features + l_features\n",
    "        assert len(features) == self.n_features\n",
    "        return features\n",
    "\n",
    "    def get_oracle(self, stack, buf, ex):\n",
    "        if len(stack) < 2:\n",
    "            return self.n_trans - 1\n",
    "\n",
    "        i0 = stack[-1]\n",
    "        i1 = stack[-2]\n",
    "        h0 = ex['head'][i0]\n",
    "        h1 = ex['head'][i1]\n",
    "        l0 = ex['label'][i0]\n",
    "        l1 = ex['label'][i1]\n",
    "\n",
    "        if self.unlabeled:\n",
    "            if (i1 > 0) and (h1 == i0):\n",
    "                return 0\n",
    "            elif (i1 >= 0) and (h0 == i1) and \\\n",
    "                 (not any([x for x in buf if ex['head'][x] == i0])):\n",
    "                return 1\n",
    "            else:\n",
    "                return None if len(buf) == 0 else 2\n",
    "        else:\n",
    "            if (i1 > 0) and (h1 == i0):\n",
    "                return l1 if (l1 >= 0) and (l1 < self.n_deprel) else None\n",
    "            elif (i1 >= 0) and (h0 == i1) and \\\n",
    "                 (not any([x for x in buf if ex['head'][x] == i0])):\n",
    "                return l0 + self.n_deprel if (l0 >= 0) and (l0 < self.n_deprel) else None\n",
    "            else:\n",
    "                return None if len(buf) == 0 else self.n_trans - 1\n",
    "\n",
    "    def create_instances(self, examples):\n",
    "        all_instances = []\n",
    "        succ = 0\n",
    "        for id, ex in enumerate(examples):\n",
    "            n_words = len(ex['word']) - 1\n",
    "\n",
    "            # arcs = {(h, t, label)}\n",
    "            stack = [0]\n",
    "            buf = [i + 1 for i in range(n_words)]\n",
    "            arcs = []\n",
    "            instances = []\n",
    "            for i in range(n_words * 2):\n",
    "                gold_t = self.get_oracle(stack, buf, ex)\n",
    "                if gold_t is None:\n",
    "                    break\n",
    "                legal_labels = self.legal_labels(stack, buf)\n",
    "                assert legal_labels[gold_t] == 1\n",
    "                instances.append((self.extract_features(stack, buf, arcs, ex),\n",
    "                                  legal_labels, gold_t))\n",
    "                if gold_t == self.n_trans - 1:\n",
    "                    stack.append(buf[0])\n",
    "                    buf = buf[1:]\n",
    "                elif gold_t < self.n_deprel:\n",
    "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
    "                    stack = stack[:-2] + [stack[-1]]\n",
    "                else:\n",
    "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
    "                    stack = stack[:-1]\n",
    "            else:\n",
    "                succ += 1\n",
    "                all_instances += instances\n",
    "\n",
    "        return all_instances\n",
    "\n",
    "    def legal_labels(self, stack, buf):\n",
    "        labels = ([1] if len(stack) > 2 else [0]) * self.n_deprel\n",
    "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel\n",
    "        labels += [1] if len(buf) > 0 else [0]\n",
    "        return labels\n",
    "\n",
    "    def parse(self, dataset, eval_batch_size=5000):\n",
    "        sentences = []\n",
    "        sentence_id_to_idx = {}\n",
    "        for i, example in enumerate(dataset):\n",
    "            n_words = len(example['word']) - 1\n",
    "            sentence = [j + 1 for j in range(n_words)]\n",
    "            sentences.append(sentence)\n",
    "            sentence_id_to_idx[id(sentence)] = i\n",
    "\n",
    "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
    "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
    "\n",
    "        UAS = all_tokens = 0.0\n",
    "        with tqdm(total=len(dataset)) as prog:\n",
    "            for i, ex in enumerate(dataset):\n",
    "                head = [-1] * len(ex['word'])\n",
    "                for h, t, in dependencies[i]:\n",
    "                    head[t] = h\n",
    "                for pred_h, gold_h, gold_l, pos in \\\n",
    "                        zip(head[1:], ex['head'][1:], ex['label'][1:], ex['pos'][1:]):\n",
    "                        assert self.id2tok[pos].startswith(P_PREFIX)\n",
    "                        pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
    "                        if (self.with_punct) or (not punct(self.language, pos_str)):\n",
    "                            UAS += 1 if pred_h == gold_h else 0\n",
    "                            all_tokens += 1\n",
    "                prog.update(i + 1)\n",
    "        UAS /= all_tokens\n",
    "        return UAS, dependencies\n",
    "\n",
    "\n",
    "class ModelWrapper(object):\n",
    "    def __init__(self, parser, dataset, sentence_id_to_idx):\n",
    "        self.parser = parser\n",
    "        self.dataset = dataset\n",
    "        self.sentence_id_to_idx = sentence_id_to_idx\n",
    "\n",
    "    def predict(self, partial_parses):\n",
    "        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dependencies,\n",
    "                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n",
    "                for p in partial_parses]\n",
    "        mb_x = np.array(mb_x).astype('int32')\n",
    "        mb_x = torch.from_numpy(mb_x).long()\n",
    "        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n",
    "\n",
    "        pred = self.parser.model(mb_x)\n",
    "        pred = pred.detach().numpy()\n",
    "        pred = np.argmax(pred + 10000 * np.array(mb_l).astype('float32'), 1)\n",
    "        pred = [\"S\" if p == 2 else (\"LA\" if p == 0 else \"RA\") for p in pred]\n",
    "        return pred\n",
    "\n",
    "\n",
    "def read_conll(in_file, lowercase=False, max_example=None):\n",
    "    examples = []\n",
    "    with open(in_file) as f:\n",
    "        word, pos, head, label = [], [], [], []\n",
    "        for line in f.readlines():\n",
    "            sp = line.strip().split('\\t')\n",
    "            if len(sp) == 10:\n",
    "                if '-' not in sp[0]:\n",
    "                    word.append(sp[1].lower() if lowercase else sp[1])\n",
    "                    pos.append(sp[4])\n",
    "                    head.append(int(sp[6]))\n",
    "                    label.append(sp[7])\n",
    "            elif len(word) > 0:\n",
    "                examples.append({'word': word, 'pos': pos, 'head': head, 'label': label})\n",
    "                word, pos, head, label = [], [], [], []\n",
    "                if (max_example is not None) and (len(examples) == max_example):\n",
    "                    break\n",
    "        if len(word) > 0:\n",
    "            examples.append({'word': word, 'pos': pos, 'head': head, 'label': label})\n",
    "    return examples\n",
    "\n",
    "\n",
    "def build_dict(keys, n_max=None, offset=0):\n",
    "    count = Counter()\n",
    "    for key in keys:\n",
    "        count[key] += 1\n",
    "    ls = count.most_common() if n_max is None \\\n",
    "        else count.most_common(n_max)\n",
    "\n",
    "    return {w[0]: index + offset for (index, w) in enumerate(ls)}\n",
    "\n",
    "\n",
    "def punct(language, pos):\n",
    "    if language == 'english':\n",
    "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
    "    elif language == 'chinese':\n",
    "        return pos == 'PU'\n",
    "    elif language == 'french':\n",
    "        return pos == 'PUNC'\n",
    "    elif language == 'german':\n",
    "        return pos in [\"$.\", \"$,\", \"$[\"]\n",
    "    elif language == 'spanish':\n",
    "        # http://nlp.stanford.edu/software/spanish-faq.shtml\n",
    "        return pos in [\"f0\", \"faa\", \"fat\", \"fc\", \"fd\", \"fe\", \"fg\", \"fh\",\n",
    "                       \"fia\", \"fit\", \"fp\", \"fpa\", \"fpt\", \"fs\", \"ft\",\n",
    "                       \"fx\", \"fz\"]\n",
    "    elif language == 'universal':\n",
    "        return pos == 'PUNCT'\n",
    "    else:\n",
    "        raise ValueError('language: %s is not supported.' % language)\n",
    "\n",
    "\n",
    "def minibatches(data, batch_size):\n",
    "    x = np.array([d[0] for d in data])\n",
    "    y = np.array([d[2] for d in data])\n",
    "    one_hot = np.zeros((y.size, 3))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return get_minibatches([x, one_hot], batch_size)\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(reduced=True):\n",
    "    config = Config()\n",
    "\n",
    "    print(\"Loading data...\",)\n",
    "    start = time.time()\n",
    "    train_set = read_conll(os.path.join(config.data_path, config.train_file),\n",
    "                           lowercase=config.lowercase)\n",
    "    dev_set = read_conll(os.path.join(config.data_path, config.dev_file),\n",
    "                         lowercase=config.lowercase)\n",
    "    test_set = read_conll(os.path.join(config.data_path, config.test_file),\n",
    "                          lowercase=config.lowercase)\n",
    "    if reduced:\n",
    "        train_set = train_set[:1000]\n",
    "        dev_set = dev_set[:500]\n",
    "        test_set = test_set[:500]\n",
    "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    print(\"Building parser...\",)\n",
    "    start = time.time()\n",
    "    parser = Parser(train_set)\n",
    "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    print(\"Loading pretrained embeddings...\",)\n",
    "    start = time.time()\n",
    "    word_vectors = {}\n",
    "    for line in open(config.embedding_file).readlines():\n",
    "        sp = line.strip().split()\n",
    "        word_vectors[sp[0]] = [float(x) for x in sp[1:]]\n",
    "    embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "    for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token]\n",
    "        elif token.lower() in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    print(\"Vectorizing data...\",)\n",
    "    start = time.time()\n",
    "    train_set = parser.vectorize(train_set)\n",
    "    dev_set = parser.vectorize(dev_set)\n",
    "    test_set = parser.vectorize(test_set)\n",
    "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    print(\"Preprocessing training data...\",)\n",
    "    start = time.time()\n",
    "    train_examples = parser.create_instances(train_set)\n",
    "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    return parser, embeddings_matrix, train_examples, dev_set, test_set,\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING\n",
      "================================================================================\n",
      "Loading data...\n",
      "took 1.94 seconds\n",
      "Building parser...\n",
      "took 0.02 seconds\n",
      "Loading pretrained embeddings...\n",
      "took 1.80 seconds\n",
      "Vectorizing data...\n",
      "took 0.07 seconds\n",
      "Preprocessing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 1.23 seconds\n",
      "took 0.01 seconds\n",
      "\n",
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.6665289681404829\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 10905660.58it/s]      \n",
      "  0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 51.06\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.3647891090561946\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 10628104.47it/s]      \n",
      "  0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 58.79\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.29579754670461017\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 11228979.48it/s]      \n",
      "  0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 61.33\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.25643947441130877\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 10522304.53it/s]      \n",
      "  0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 65.30\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 27/48 [00:03<00:02,  7.69it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3b7db3e8134b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-695c139b8220>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(parser, train_data, dev_data, output_path, batch_size, n_epochs, lr)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {:} out of {:}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mdev_UAS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_for_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdev_UAS\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_dev_UAS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mbest_dev_UAS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_UAS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-695c139b8220>\u001b[0m in \u001b[0;36mtrain_for_epoch\u001b[0;34m(parser, train_data, dev_data, optimizer, loss_func, batch_size)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/a2/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/a2/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Note: Set debug to False, when training on entire corpus\n",
    "    debug = True\n",
    "    # debug = False\n",
    "\n",
    "#     assert(torch.__version__ == \"1.0.0\"),  \"Please install torch version 1.0.0\"\n",
    "\n",
    "    print(80 * \"=\")\n",
    "    print(\"INITIALIZING\")\n",
    "    print(80 * \"=\")\n",
    "    parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(debug)\n",
    "\n",
    "    start = time.time()\n",
    "    model = ParserModel(embeddings)\n",
    "    parser.model = model\n",
    "    print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
    "\n",
    "    print(80 * \"=\")\n",
    "    print(\"TRAINING\")\n",
    "    print(80 * \"=\")\n",
    "    output_dir = \"results/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
    "    output_path = output_dir + \"model.weights\"\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005)\n",
    "\n",
    "    if not debug:\n",
    "        print(80 * \"=\")\n",
    "        print(\"TESTING\")\n",
    "        print(80 * \"=\")\n",
    "        print(\"Restoring the best model weights found on the dev set\")\n",
    "        parser.model.load_state_dict(torch.load(output_path))\n",
    "        print(\"Final evaluation on test set\",)\n",
    "        parser.model.eval()\n",
    "        UAS, dependencies = parser.parse(test_data)\n",
    "        print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "        print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
